<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.1">Jekyll</generator><link href="https://liguiye.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://liguiye.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2022-12-15T22:07:18+00:00</updated><id>https://liguiye.github.io/feed.xml</id><title type="html">blank</title><entry><title type="html">Diffusion models</title><link href="https://liguiye.github.io/blog/2022/Diffusion-models/" rel="alternate" type="text/html" title="Diffusion models"/><published>2022-12-14T15:59:00+00:00</published><updated>2022-12-14T15:59:00+00:00</updated><id>https://liguiye.github.io/blog/2022/Diffusion-models</id><content type="html" xml:base="https://liguiye.github.io/blog/2022/Diffusion-models/"><![CDATA[<p><em>TOC</em></p> <ul id="markdown-toc"> <li><a href="#markov-chain" id="markdown-toc-markov-chain">Markov chain</a></li> <li><a href="#main-idea-of-diffusion-model" id="markdown-toc-main-idea-of-diffusion-model">Main idea of Diffusion Model</a> <ul> <li><a href="#forward-process-or-diffusion-process" id="markdown-toc-forward-process-or-diffusion-process"><em>Forward process</em> (or <em>diffusion process</em>)</a></li> <li><a href="#reverse-process-or-reverse-diffusion-process" id="markdown-toc-reverse-process-or-reverse-diffusion-process"><em>Reverse process</em> (or <em>reverse diffusion process</em>)</a></li> </ul> </li> <li><a href="#benefits-of-diffusion-models" id="markdown-toc-benefits-of-diffusion-models">Benefits of Diffusion Models</a></li> <li><a href="#training" id="markdown-toc-training">Training</a></li> <li><a href="#summary" id="markdown-toc-summary">Summary</a></li> </ul> <p><em>Reference:</em></p> <p>Official links:</p> <ol> <li> <p>DDPM (NeurIPS 2020)</p> <p>Paper: <a href="https://arxiv.org/abs/2006.11239">Denoising Diffusion Probabilistic Models</a></p> <p>Github: <a href="https://github.com/hojonathanho/diffusion">https://github.com/hojonathanho/diffusion</a></p> <p>Website: <a href="https://hojonathanho.github.io/diffusion/">https://hojonathanho.github.io/diffusion/</a></p> </li> <li> <p>Improved DDPM (ICML 2021)</p> <p>Paper: <a href="https://arxiv.org/abs/2102.09672">Improved Denoising Diffusion Probabilistic Models</a></p> <p>Github: <a href="https://github.com/openai/improved-diffusion">https://github.com/openai/improved-diffusion</a></p> </li> <li> <p>Guided Diffusion Models (NeurIPS 2021)</p> <p>Paper: <a href="https://arxiv.org/pdf/2105.05233.pdf">Diffusion Models Beat GANs on Image Synthesis</a></p> <p>Github: <a href="https://github.com/openai/guided-diffusion">https://github.com/openai/guided-diffusion</a></p> </li> </ol> <p>Blog:</p> <ol> <li><a href="https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/">Introduction to Diffusion Models for Machine Learning</a></li> <li><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">What are Diffusion Models?</a></li> </ol> <h2 id="markov-chain">Markov chain</h2> <p>Quote from <a href="https://en.wikipedia.org/wiki/Markov_chain#:~:text=A%20Markov%20chain%20or%20Markov,the%20state%20of%20affairs%20now.%22">Wikipedia</a>:</p> <p>A <strong>Markov chain</strong> or <strong>Markov process</strong> is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. Informally, this may be thought of as, “What happens next depends only on the state of affairs now”.</p> <h2 id="main-idea-of-diffusion-model">Main idea of Diffusion Model</h2> <p>Diffusion models works by <strong>destroying training data</strong> through the successive addition if Gaussian noise, and then <strong>learning to recover</strong> the data by reversing this noising process. After training, we can use the Diffusion Model to generate data by simply <strong>passing randomly sampled noise through the learned denoising process</strong>.</p> <h3 id="forward-process-or-diffusion-process"><em>Forward process</em> (or <em>diffusion process</em>)</h3> <p>Specifically, a Diffusion Model is a latent variable model which maps to the latent space using a fixed Markov chain. This chain gradually adds noise to the data in order to obtain the <strong>approximate posterior</strong> \(q(X_{1:T}\vert X_0)\), where \(X_1,\ldots,X_T\) are the latent variables with the same dimensionality as \(X_0\). See figure below.</p> <p><img src="https://www.assemblyai.com/blog/content/images/size/w1000/2022/05/image.png" alt="The Markov chain manifested for image data." width="600pt"/></p> <p>A parameterization of the forward process (combing Markov assumption):</p> <p>\begin{equation} \label{eq:forward-process} q(X_{1:T} \vert X_0) := \prod^T_{t=1} q(X_t \vert X_{t-1}) := \prod^T_{t=1} \mathcal{N}(X_t;\sqrt{1-\beta_t}X_{t-1},\beta_t\mathbf{I}) \end{equation}</p> <p>where \(\beta_1,\dots,\beta_T\) is a variance schedule (either learned or fixed) which, if well-behaved, <strong>ensures that \(X_T\) is nearly an isotropic Gaussian for sufficiently large \(T\)</strong>.</p> <p><em>Let’s skip the proof for now.</em></p> <h3 id="reverse-process-or-reverse-diffusion-process"><em>Reverse process</em> (or <em>reverse diffusion process</em>)</h3> <p>Ultimately, the image is asymptotically transformed to pure Gaussian noise. The goal of training a diffusion model is to <strong>learn the reverse process</strong>, i.e. training \(p_\theta (X_{t-1} \vert X_t)\). See figure below, by traversing backwards along this chain, we can generate new data.</p> <p><img src="https://www.assemblyai.com/blog/content/images/size/w1000/2022/05/image-1.png" alt="The reverse process of the Markov chain." width="600pt"/></p> <p>Starting with the pure Gaussian noise \(p(X_T)=\mathcal{N}(X_T;\mathbf{0},\mathbf{I})\), the model learns the joint distribution \(p_\theta(X_0;T)\) as:</p> <p>\begin{equation} p_\theta(X_0;T) := p(X_T) \prod^T_{t=1} p_\theta(X_{t-1}\vert X_t) := p(X_T) \prod^T_{t=1} \mathcal{N}(X_{t-1};\mu_\theta(x_t,t),\varSigma_\theta(X_t,t)) \end{equation}</p> <p>where the time-dependent parameters of the Gaussian transitions are learned.</p> <h2 id="benefits-of-diffusion-models">Benefits of Diffusion Models</h2> <ol> <li>Diffusion Models currently produce State-of-the-Art image quality.</li> <li>Not requiring adversarial training.</li> <li>Scalability and parallelizability.</li> </ol> <h2 id="training">Training</h2> <p>A Diffusion Model is trained by <strong>finding the reverse Markov transitions that maximize the likelihood of the training data</strong>. In practice, training equivalently consists of minimizing the variational upper bound on the negative log likelihood.</p> <p>\begin{equation} \mathbb{E} [= \log p_\theta (X_0)] \leqslant \mathbb{E}_ {q} [-\log \frac{p_\theta (X_{0:T})}{q(X_{1:T} \vert X_0)}] =: L_{vlb} \end{equation}</p> <p>Variational lower bound \(L_{vlb}\) is technically an upper bound (the negative of the ELBO) which we are trying to minimize. We will try to rewrite the \(L_{vlb}\) in terms of Kullback-Leibler (KL) Divergences because the transition distributions in the Markov chain are Gaussians, and <strong>the KL divergence between Gaussians has a closed form</strong>.</p> <p>\begin{equation} D_{KL}(P\parallel Q) = \int_{-\infty}^{\infty} p(x)\log(\frac{p(x)}{q(x)}) dx \end{equation}</p> <p>Casting \(L_{vlb}\) in terms of KL Divergences</p> <p>\begin{equation} L_{vlb} := L_0 + L_1 + \ldots + L_{T-1} + L_T \end{equation}</p> <p>where</p> <p>\begin{equation} L_0 := -\log p_\theta(X_0 \vert X_1) \end{equation}</p> <p>\begin{equation} L_{t-1} := D_{KL}(q(X_{t-1} \vert X_t,X_0)\parallel p_\theta(X_{t-1} \vert X_t)) \end{equation}</p> <p>\begin{equation} L_T := D_{KL}(q(X_T \vert X_0)\parallel p(X_T)) \end{equation}</p> <p><em>Let’s skip the proof for now.</em></p> <p>Conditioning the forward process posterior on \(X_0\) in \(L_{t-1}\) results in a tractable form that leads to <strong>all KL divergences being comparisons between Gaussians</strong>. This means that the divergences can be exactly calculated with closed-form expressions rather than with Monte Carlo estimates.</p> <h2 id="summary">Summary</h2> <ol> <li>Diffusion Models are <strong>highly flexible</strong> and allow for any architecture whose input and output dimensionality are the same to be used. Many implementations use <strong>U-Net-like</strong> architectures.</li> <li>The <strong>training objective</strong> is to maximize the likelihood of t he training data. This is manifested as tuning the model parameters to <strong>minimize the variational upper bound of the negative log likelihood of the data</strong>.</li> <li>Almost all terms in the objective function can be cast as <strong>KL Divergences</strong> as a result of the Markov assumption. This values <strong>become tenable to calculated</strong> given that we are using Gaussians, therefore omitting the need to perform MonteCarlo approximation.</li> <li>A discrete decoder is used to obatin log likelihoods across pixel values as the last step in the reverse diffusion process.</li> </ol>]]></content><author><name></name></author><category term="generative-model"/><category term="deep-learning"/><category term="diffusion-model"/><summary type="html"><![CDATA[TOC]]></summary></entry><entry><title type="html">Score based methods</title><link href="https://liguiye.github.io/blog/2022/Score-based-methods/" rel="alternate" type="text/html" title="Score based methods"/><published>2022-12-13T15:59:00+00:00</published><updated>2022-12-13T15:59:00+00:00</updated><id>https://liguiye.github.io/blog/2022/Score-based-methods</id><content type="html" xml:base="https://liguiye.github.io/blog/2022/Score-based-methods/"><![CDATA[<p><em>TOC</em></p> <ul id="markdown-toc"> <li><a href="#1-score-based-generative-modeling" id="markdown-toc-1-score-based-generative-modeling">1. Score-based generative modeling</a> <ul> <li><a href="#score-matching-for-score-estimation" id="markdown-toc-score-matching-for-score-estimation">score matching for score estimation</a> <ul> <li><a href="#11-denoising-score-matching" id="markdown-toc-11-denoising-score-matching">1.1. Denoising score matching</a></li> <li><a href="#12-sliced-score-matching" id="markdown-toc-12-sliced-score-matching">1.2. Sliced score matching</a></li> </ul> </li> </ul> </li> <li><a href="#2-sampling-with-langevin-dynamics" id="markdown-toc-2-sampling-with-langevin-dynamics">2. Sampling with Langevin Dynamics</a></li> <li><a href="#3-challenges" id="markdown-toc-3-challenges">3. Challenges</a></li> <li><a href="#4-contributions-of-ncsn" id="markdown-toc-4-contributions-of-ncsn">4. Contributions of NCSN</a></li> </ul> <p><em>Reference:</em></p> <p>Official links:</p> <p>Noise Conditional Score Networks (NCSN) (NeurIPS 2019)</p> <ol> <li>Paper: <a href="https://arxiv.org/abs/1907.05600">Generative Modeling by Estimating Gradients of the Data Distribution</a></li> <li>Blog: <a href="https://yang-song.net/blog/">https://yang-song.net/blog/</a></li> <li>Github: <a href="https://github.com/ermongroup/ncsn">https://github.com/ermongroup/ncsn</a></li> </ol> <h2 id="1-score-based-generative-modeling">1. Score-based generative modeling</h2> <p>Given a probability density function (PDF) \(p(x)\), the ‘score’ is defined as \(\triangledown_x \log p(x)\), or the gradient of the log-likelihood of the object \(x\) w.r.t the input dimensions \(x\), notably not w.r.t the model parameters \(\theta\). We will assume that PDFs are continuous random variables. The score is a vector field of the gradient at any point \(x\). This gradient of \(\log p(x)\) tells us the directions in which to move if we want to increase the likelihood as much as possible.</p> <p>The <strong>score-based network</strong> \(s_\theta: \mathbb{R}^D \rightarrow \mathbb{R}^D\) is a neural network parameterized by \(\theta\), which will be trained to approximate the score of \(p_ {data}(x)\) ( \(\triangledown_x\log p(x)\) ). The framework of score-based generative modeling has two ingredients: <em>score matching</em> and <em>Langevin dynamics</em>.</p> <h3 id="score-matching-for-score-estimation">score matching for score estimation</h3> <p>Using score matching, we can directly train a score network \(s_\theta(x)\) to estimate \(\triangledown_x \log p(x)\) without training a model to estimate \(p_ {data}(x)\) first.</p> <p>The objective minimizes \(\frac{1}{2} \mathbb{E}_ {p_ {data}(x)}[\parallel s_\theta(x)-\triangledown_x \log p_ {data}(x)\parallel_2^2]\), which can be shown equivalent to the following up to a constant</p> <p>\begin{equation} \mathbb{E}_ {p_ {data}(x)}[\text{tr}(\triangledown_xs_\theta(x))+\frac{1}{2}\parallel s_\theta(x)\parallel _2^2] \end{equation}</p> <p>where \(\triangledown_xs_\theta(x)\) is the Jacobian (first-order partial derivatives) of \(s_\theta(x)\). Note that the trace of a square matrix \(A\), denoted \(\text{tr}(A)\), is defined to be the sum of elements on the main diagonal (from the upper left to the lower right) of \(A\).</p> <p>In practice, the expectation over \(p_ {data}(x)\) can be quickly estimated using data samples. However, score matching is not scalable to deep networks and high dimensional data due to the computation of \(\text{tr}(\triangledown_xs_\theta(x))\). Below are the two popular methods for large scale score matching.</p> <h4 id="11-denoising-score-matching">1.1. Denoising score matching</h4> <p>A variant of score matching that completely circumvents \(\text{tr}(\triangledown_xs_\theta(x))\). It first perturbs the data point \(x\) with a pre-specified noise distribution \(q_\sigma(\tilde{x}\vert x)\) and then employs score matching to estimate the score of the perturbed data distribution \(q_\sigma(\tilde{x}) \triangleq \int q_\sigma(\tilde{x}\vert x)p_ {data}(x)dx\). The objective was proved equivalent to the following:</p> <p>\begin{equation} \frac{1}{2} \mathbb{E}_ {q_\sigma(\tilde{x}\vert x)p_ {data}(x)}[\parallel s_\theta(\tilde{x})-\triangledown_ {\tilde{x}}\log q_\sigma(\tilde{x}\vert x)\parallel _2^2] \end{equation}</p> <h4 id="12-sliced-score-matching">1.2. Sliced score matching</h4> <p>Sliced score matching uses random projections to approximate \(\text{tr}(\triangledown_xs_\theta(x))\) in score matching. The objective is</p> <p>\begin{equation} \mathbb{E}_ {p_v}\mathbb{E}_ {p_ {data}}[v^T \triangledown_x s_\theta(x)v + \frac{1}{2} \parallel s_\theta(x)\parallel _2^2] \end{equation}</p> <p>where \(p_v\) is a simple distribution of random vectors, e.g., the multivariate standard normal. The term \(v^T \triangledown_x s_\theta(x)v\) can be efficiently computed by forward mode auto-differentiation. Unlike denoising score matching which estimates the scores of <em>perturbed</em> data, sliced score matching provides score estimation for the original <em>unperturbed</em> data distribution, but requires around four times more computation due to the forward mode auto-differentiation.</p> <h2 id="2-sampling-with-langevin-dynamics">2. Sampling with Langevin Dynamics</h2> <p>Langevin Monte Carlo is a Markov Chain Monte Carlo (MCMC) method for obtaining random samples from probability distributions for which direct sampling is difficult. The goal is to “follow the gradient but add a bit of noise” so as to not get stuck at the local optima regions and thus we are able to explore the distribution and sample from it. It approximately works by gradually moving a random initial sample to high density regions along the (estimated) vector field of scores.</p> <p>Langevin dynamics can produce samples from a probability density \(p(x)\) using only the score function \(\triangledown_x \log p_ {data}(x)\). Given a fixed step size \(\epsilon &gt; 0\), and an initial value \(\tilde{x}_0 \sim \pi(x)\) with \(\pi\) being a prior distribution, the Langevin method recursively computes the following</p> <p>\begin{equation} \tilde{x}_ t = \tilde{x}_ {t-1} + \frac{\epsilon}{2} \triangledown_x \log p(\tilde{x}_ {t-1}) + \sqrt{\epsilon} z_t \end{equation}</p> <p>where \(z_t \sim \mathcal{N}(0,I)\). The distribution of \(\tilde{x}_T\) equals \(p(x)\) when \(\epsilon \rightarrow 0\) and \(T \rightarrow \infty\), in which case \(\tilde{x}_T\) becomes an exact sample from \(p(x)\) under some regularity conditions. We usually assume the error is negligible when \(\epsilon\) is small and \(T\) is large.</p> <hr/> <p>Note that sampling from this equation only requires the score function \(\triangledown_x \log p_ {data}(x)\). Therefore, in order to obtain samples from \(p_ {data}(x)\), we can first train our score network such that \(s_\theta(x) \approx \triangledown_x \log p_ {data}(x)\) and then approximately obtain samples with Langevin dynamics using \(s_\theta(x)\). This is the key idea of the <em>score-based generative modeling</em>.</p> <hr/> <h2 id="3-challenges">3. Challenges</h2> <ol> <li>If the <strong>data distribution is</strong> supported on a <strong>low dimensional manifold</strong> - it is often assumed for many real world datasets - the <strong>score will be undefined in the ambient space</strong>, and <strong>score matching will fail to provide a consistent score estimator</strong>. The score matching objective provides a consistent score estimator only when the support of the data distribution is the whole space.</li> <li> <p>The scarcity of training data in <strong>low data density regions</strong>, e.g., far from the manifold, <strong>hinders the accuracy of score estimation</strong> and <strong>slows down the mixing of Langevin dynamics sampling</strong>. Since Langevin dynamics will often be initialized in low-density regions of the data distribution, inaccurate score estimation in these regions will negatively affect the sampling process.</p> <p align="center" id="negative-effect-low-density"> <img src="../../../assets/img/blog/NCSN-negative_effect_of_low_density.png" alt="Inaccurate score estimation with score matching" width="300pt"/> <img src="../../../assets/img/blog/NCSN-low_density_pitfalls.jpg" alt="Inaccurate score estimation with score matching" width="600pt"/> </p> <p>As the <a href="#negative-effect-low-density">figure</a> demonstrates, score estimation is only reliable in the immediate vicinity of the models of \(p_ {data}\), where the data density is high.</p> </li> <li> <p>Mixing can be difficult because of the need of traversing low density regions to transition between models of the distribution. In other words, when two models of the data distribution are separated by low density regions, Langevin dynamics will not be able to correctly recover the relative weights of these two modes in reasonable time, and therefore might not converge to the true distribution.</p> <p align="center"> <img src="../../../assets/img/blog/NCSN-slow_mixing_of_Langevin_dynamics.png" alt="Slow mixing of Langevin dynamics" width="600pt"/> </p> </li> </ol> <h2 id="4-contributions-of-ncsn">4. Contributions of NCSN</h2> <ol> <li> <p>Propose to <strong>perturb the data with random Gaussian noise of various magnitudes</strong>.</p> <p>Adding random noise ensues the resulting distribution does not collapse to a low dimensional manifold. Large noise levels will produce samples in low density regions of the original (unperturbed) data distribution, thus improving score estimation.</p> <p align="center" id="perturb_data_with_noise"> <img src="../../../assets/img/blog/NCSN-perturb_data_with_noise.png" alt="Perturb data with random Gaussian noise" width="400pt"/> </p> <p>As the <a href="#perturb_data_with_noise">figure</a> (left) shows, when trained on the original CIFAR-10 images, the sliced score matching loss first decreases and then fluctuates irregularly. In contrast, if wee perturb the data with a small Gaussian noise (such that the perturbed data distribution has full support over \(\mathbb{R}^D\)), the loss curve will converge (right panel). Note that the Gaussian noise \(\mathcal{N}(0, 0.0001)\) we impose is very small for images with pixel values in the range \([0,1]\), and is almost indistinguishable to human eyes.</p> </li> <li> <p>Train a single score network conditioned on the noise level and estimate the scores at all noise magnitudes.</p> <p>Let \(\{ \sigma_i \}_ {i=1}^L\) be a positive geometric sequence that satisfies \(\frac{\sigma_1}{\sigma_2} = \ldots = \frac{\sigma_ {L-1}}{\sigma_L} &gt; 1\).</p> <p>Let \(q_\sigma(x) \triangleq \int p_ {data}(t) \mathcal{N}(x\vert t,\sigma^2I)dt\) denote the perturbed data distribution.</p> <p>We choose the noise levels \(\{\sigma_i\}_ {i=1}^L\) such that \(\sigma_1\) is large enough to mitigate the difficulties discussed before, and \(\sigma_L\) is small enough to minimize the effect on data. The conditional score network \(s_\theta(x,\sigma)\) is trained to jointly estimate the scores of all perturbed data distributions, i.e., \(\forall_\sigma \in \{\sigma_i\}_ {i=1}^L : s_\theta(x,\sigma) \approx \triangledown_x \log q_\sigma (x)\). Note that \(s_\theta(x,\sigma) \in \mathbb{R}^D\) when \(x \in \mathbb{R}^D\).</p> </li> <li> <p>Propose <strong>an annealed version of Langevin dynamics</strong>, where we initially use scores corresponding to the highest noise level, and gradually anneal down the noise level until it is small enough to be indistinguishable from the original data distribution.</p> </li> </ol>]]></content><author><name></name></author><category term="generative-model"/><category term="deep-learning"/><summary type="html"><![CDATA[TOC]]></summary></entry><entry><title type="html">a post with redirect</title><link href="https://liguiye.github.io/blog/2022/redirect/" rel="alternate" type="text/html" title="a post with redirect"/><published>2022-02-01T17:39:00+00:00</published><updated>2022-02-01T17:39:00+00:00</updated><id>https://liguiye.github.io/blog/2022/redirect</id><content type="html" xml:base="https://liguiye.github.io/blog/2022/redirect/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[you can also redirect to assets like pdf]]></summary></entry><entry><title type="html">a distill-style blog post</title><link href="https://liguiye.github.io/blog/2021/distill/" rel="alternate" type="text/html" title="a distill-style blog post"/><published>2021-05-22T00:00:00+00:00</published><updated>2021-05-22T00:00:00+00:00</updated><id>https://liguiye.github.io/blog/2021/distill</id><content type="html" xml:base="https://liguiye.github.io/blog/2021/distill/"><![CDATA[<h2 id="equations">Equations</h2> <p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine. You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>. If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p> <p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph. Here is an example:</p> \[\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)\] <p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p> <hr/> <h2 id="citations">Citations</h2> <p>Citations are then used in the article body with the <code class="language-plaintext highlighter-rouge">&lt;d-cite&gt;</code> tag. The key attribute is a reference to the id provided in the bibliography. The key attribute can take multiple ids, separated by commas.</p> <p>The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover). If you have an appendix, a bibliography is automatically created and populated in it.</p> <p>Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover. However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.</p> <hr/> <h2 id="footnotes">Footnotes</h2> <p>Just wrap the text you would like to show up in a footnote in a <code class="language-plaintext highlighter-rouge">&lt;d-footnote&gt;</code> tag. The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote></p> <hr/> <h2 id="code-blocks">Code Blocks</h2> <p>Syntax highlighting is provided within <code class="language-plaintext highlighter-rouge">&lt;d-code&gt;</code> tags. An example of inline code snippets: <code class="language-plaintext highlighter-rouge">&lt;d-code language="html"&gt;let x = 10;&lt;/d-code&gt;</code>. For larger blocks of code, add a <code class="language-plaintext highlighter-rouge">block</code> attribute:</p> <d-code block="" language="javascript"> var x = 25; function(x) { return x * x; } </d-code> <p><strong>Note:</strong> <code class="language-plaintext highlighter-rouge">&lt;d-code&gt;</code> blocks do not look good in the dark mode. You can always use the default code-highlight using the <code class="language-plaintext highlighter-rouge">highlight</code> liquid tag:</p> <figure class="highlight"><pre><code class="language-javascript" data-lang="javascript"><span class="kd">var</span> <span class="nx">x</span> <span class="o">=</span> <span class="mi">25</span><span class="p">;</span>
<span class="kd">function</span><span class="p">(</span><span class="nx">x</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">return</span> <span class="nx">x</span> <span class="o">*</span> <span class="nx">x</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure> <hr/> <h2 id="layouts">Layouts</h2> <p>The main text column is referred to as the body. It is the assumed layout of any direct descendants of the <code class="language-plaintext highlighter-rouge">d-article</code> element.</p> <div class="fake-img l-body"> <p>.l-body</p> </div> <p>For images you want to display a little larger, try <code class="language-plaintext highlighter-rouge">.l-page</code>:</p> <div class="fake-img l-page"> <p>.l-page</p> </div> <p>All of these have an outset variant if you want to poke out from the body text a little bit. For instance:</p> <div class="fake-img l-body-outset"> <p>.l-body-outset</p> </div> <div class="fake-img l-page-outset"> <p>.l-page-outset</p> </div> <p>Occasionally you’ll want to use the full browser width. For this, use <code class="language-plaintext highlighter-rouge">.l-screen</code>. You can also inset the element a little from the edge of the browser by using the inset variant.</p> <div class="fake-img l-screen"> <p>.l-screen</p> </div> <div class="fake-img l-screen-inset"> <p>.l-screen-inset</p> </div> <p>The final layout is for marginalia, asides, and footnotes. It does not interrupt the normal flow of <code class="language-plaintext highlighter-rouge">.l-body</code> sized text except on mobile screen sizes.</p> <div class="fake-img l-gutter"> <p>.l-gutter</p> </div> <hr/> <h2 id="other-typography">Other Typography?</h2> <p>Emphasis, aka italics, with <em>asterisks</em> (<code class="language-plaintext highlighter-rouge">*asterisks*</code>) or <em>underscores</em> (<code class="language-plaintext highlighter-rouge">_underscores_</code>).</p> <p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>.</p> <p>Combined emphasis with <strong>asterisks and <em>underscores</em></strong>.</p> <p>Strikethrough uses two tildes. <del>Scratch this.</del></p> <ol> <li>First ordered list item</li> <li>Another item ⋅⋅* Unordered sub-list.</li> <li>Actual numbers don’t matter, just that it’s a number ⋅⋅1. Ordered sub-list</li> <li>And another item.</li> </ol> <p>⋅⋅⋅You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we’ll use three here to also align the raw Markdown).</p> <p>⋅⋅⋅To have a line break without a paragraph, you will need to use two trailing spaces.⋅⋅ ⋅⋅⋅Note that this line is separate, but within the same paragraph.⋅⋅ ⋅⋅⋅(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.)</p> <ul> <li>Unordered list can use asterisks</li> <li>Or minuses</li> <li>Or pluses</li> </ul> <p><a href="https://www.google.com">I’m an inline-style link</a></p> <p><a href="https://www.google.com" title="Google's Homepage">I’m an inline-style link with title</a></p> <p><a href="https://www.mozilla.org">I’m a reference-style link</a></p> <p><a href="../blob/master/LICENSE">I’m a relative reference to a repository file</a></p> <p><a href="http://slashdot.org">You can use numbers for reference-style link definitions</a></p> <p>Or leave it empty and use the <a href="http://www.reddit.com">link text itself</a>.</p> <p>URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or <a href="http://www.example.com">http://www.example.com</a> and sometimes example.com (but not on Github, for example).</p> <p>Some text to show that the reference links can follow later.</p> <p>Here’s our logo (hover to see the title text):</p> <p>Inline-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 1"/></p> <p>Reference-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 2"/></p> <p>Inline <code class="language-plaintext highlighter-rouge">code</code> has <code class="language-plaintext highlighter-rouge">back-ticks around</code> it.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">s</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">JavaScript syntax highlighting</span><span class="dl">"</span><span class="p">;</span>
<span class="nx">alert</span><span class="p">(</span><span class="nx">s</span><span class="p">);</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">=</span> <span class="s">"Python syntax highlighting"</span>
<span class="k">print</span> <span class="n">s</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No language indicated, so no syntax highlighting.
But let's throw in a &lt;b&gt;tag&lt;/b&gt;.
</code></pre></div></div> <p>Colons can be used to align columns.</p> <table> <thead> <tr> <th>Tables</th> <th style="text-align: center">Are</th> <th style="text-align: right">Cool</th> </tr> </thead> <tbody> <tr> <td>col 3 is</td> <td style="text-align: center">right-aligned</td> <td style="text-align: right">$1600</td> </tr> <tr> <td>col 2 is</td> <td style="text-align: center">centered</td> <td style="text-align: right">$12</td> </tr> <tr> <td>zebra stripes</td> <td style="text-align: center">are neat</td> <td style="text-align: right">$1</td> </tr> </tbody> </table> <p>There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don’t need to make the raw Markdown line up prettily. You can also use inline Markdown.</p> <table> <thead> <tr> <th>Markdown</th> <th>Less</th> <th>Pretty</th> </tr> </thead> <tbody> <tr> <td><em>Still</em></td> <td><code class="language-plaintext highlighter-rouge">renders</code></td> <td><strong>nicely</strong></td> </tr> <tr> <td>1</td> <td>2</td> <td>3</td> </tr> </tbody> </table> <blockquote> <p>Blockquotes are very handy in email to emulate reply text. This line is part of the same quote.</p> </blockquote> <p>Quote break.</p> <blockquote> <p>This is a very long line that will still be quoted properly when it wraps. Oh boy let’s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can <em>put</em> <strong>Markdown</strong> into a blockquote.</p> </blockquote> <p>Here’s a line for us to start with.</p> <p>This line is separated from the one above by two newlines, so it will be a <em>separate paragraph</em>.</p> <p>This line is also a separate paragraph, but… This line is only separated by a single newline, so it’s a separate line in the <em>same paragraph</em>.</p>]]></content><author><name>Albert Einstein</name></author><summary type="html"><![CDATA[an example of a distill-style blog post and main elements]]></summary></entry><entry><title type="html">a post with github metadata</title><link href="https://liguiye.github.io/blog/2020/github-metadata/" rel="alternate" type="text/html" title="a post with github metadata"/><published>2020-09-28T21:01:00+00:00</published><updated>2020-09-28T21:01:00+00:00</updated><id>https://liguiye.github.io/blog/2020/github-metadata</id><content type="html" xml:base="https://liguiye.github.io/blog/2020/github-metadata/"><![CDATA[<p>A sample blog page that demonstrates the accessing of github meta data.</p> <h2 id="what-does-github-metadata-do">What does Github-MetaData do?</h2> <ul> <li>Propagates the site.github namespace with repository metadata</li> <li>Setting site variables : <ul> <li>site.title</li> <li>site.description</li> <li>site.url</li> <li>site.baseurl</li> </ul> </li> <li>Accessing the metadata - duh.</li> <li>Generating edittable links.</li> </ul> <h2 id="additional-reading">Additional Reading</h2> <ul> <li>If you’re recieving incorrect/missing data, you may need to perform a Github API<a href="https://github.com/jekyll/github-metadata/blob/master/docs/authentication.md"> authentication</a>.</li> <li>Go through this <a href="https://jekyll.github.io/github-metadata/">README</a> for more details on the topic.</li> <li><a href="https://github.com/jekyll/github-metadata/blob/master/docs/site.github.md">This page</a> highlights all the feilds you can access with github-metadata. <br/></li> </ul> <h2 id="example-metadata">Example MetaData</h2> <ul> <li>Host Name :</li> <li>URL :</li> <li>BaseURL :</li> <li>Archived :</li> <li>Contributors :</li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="external-services"/><summary type="html"><![CDATA[a quick run down on accessing github metadata.]]></summary></entry><entry><title type="html">a post with math</title><link href="https://liguiye.github.io/blog/2015/math/" rel="alternate" type="text/html" title="a post with math"/><published>2015-10-20T15:12:00+00:00</published><updated>2015-10-20T15:12:00+00:00</updated><id>https://liguiye.github.io/blog/2015/math</id><content type="html" xml:base="https://liguiye.github.io/blog/2015/math/"><![CDATA[<p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine. You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>. If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p> <p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph. Here is an example:</p> \[\sum_{k=1}^\infty |\langle x, e_k \rangle|^2 \leq \|x\|^2\] <p>You can also use <code class="language-plaintext highlighter-rouge">\begin{equation}...\end{equation}</code> instead of <code class="language-plaintext highlighter-rouge">$$</code> for display mode math. MathJax will automatically number equations:</p> <p>\begin{equation} \label{eq:cauchy-schwarz} \left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right) \end{equation}</p> <p>and by adding <code class="language-plaintext highlighter-rouge">\label{...}</code> inside the equation environment, we can now refer to the equation using <code class="language-plaintext highlighter-rouge">\eqref</code>.</p> <p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="math"/><summary type="html"><![CDATA[an example of a blog post with some math]]></summary></entry><entry><title type="html">a post with code</title><link href="https://liguiye.github.io/blog/2015/code/" rel="alternate" type="text/html" title="a post with code"/><published>2015-07-15T15:09:00+00:00</published><updated>2015-07-15T15:09:00+00:00</updated><id>https://liguiye.github.io/blog/2015/code</id><content type="html" xml:base="https://liguiye.github.io/blog/2015/code/"><![CDATA[<p>This theme implements a built-in Jekyll feature, the use of Rouge, for syntax highlighting. It supports more than 100 languages. This example is in C++. All you have to do is wrap your code in a liquid tag:</p> <p>{% highlight c++ linenos %} <br/> code code code <br/> {% endhighlight %}</p> <p>The keyword <code class="language-plaintext highlighter-rouge">linenos</code> triggers display of line numbers. Produces something like this:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre></td><td class="code"><pre><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="k">const</span> <span class="err">\</span><span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
    <span class="n">string</span> <span class="n">myString</span><span class="p">;</span>

    <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"input a string: "</span><span class="p">;</span>
    <span class="n">getline</span><span class="p">(</span><span class="n">cin</span><span class="p">,</span> <span class="n">myString</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">length</span> <span class="o">=</span> <span class="n">myString</span><span class="p">.</span><span class="n">length</span><span class="p">();</span>

    <span class="kt">char</span> <span class="n">charArray</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">char</span> <span class="o">*</span> <span class="p">[</span><span class="n">length</span><span class="p">];</span>

    <span class="n">charArray</span> <span class="o">=</span> <span class="n">myString</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">charArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></figure>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[an example of a blog post with some code]]></summary></entry><entry><title type="html">a post with images</title><link href="https://liguiye.github.io/blog/2015/images/" rel="alternate" type="text/html" title="a post with images"/><published>2015-05-15T21:01:00+00:00</published><updated>2015-05-15T21:01:00+00:00</updated><id>https://liguiye.github.io/blog/2015/images</id><content type="html" xml:base="https://liguiye.github.io/blog/2015/images/"><![CDATA[<p>This is an example post with image galleries.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/9-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/9-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/9-1400.webp"/> <img src="/assets/img/9.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/7-1400.webp"/> <img src="/assets/img/7.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all. </div> <p>Images can be made zoomable. Simply add <code class="language-plaintext highlighter-rouge">data-zoomable</code> to <code class="language-plaintext highlighter-rouge">&lt;img&gt;</code> tags that you want to make zoomable.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/8-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/8-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/8-1400.webp"/> <img src="/assets/img/8.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/10-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/10-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/10-1400.webp"/> <img src="/assets/img/10.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The rest of the images in this post are all zoomable, arranged into different mini-galleries.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/11-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/11-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/11-1400.webp"/> <img src="/assets/img/11.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/12-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/12-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/12-1400.webp"/> <img src="/assets/img/12.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/7-1400.webp"/> <img src="/assets/img/7.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[this is what included images could look like]]></summary></entry><entry><title type="html">a post with formatting and links</title><link href="https://liguiye.github.io/blog/2015/formatting-and-links/" rel="alternate" type="text/html" title="a post with formatting and links"/><published>2015-03-15T16:40:16+00:00</published><updated>2015-03-15T16:40:16+00:00</updated><id>https://liguiye.github.io/blog/2015/formatting-and-links</id><content type="html" xml:base="https://liguiye.github.io/blog/2015/formatting-and-links/"><![CDATA[<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p> <h4 id="hipster-list">Hipster list</h4> <ul> <li>brunch</li> <li>fixie</li> <li>raybans</li> <li>messenger bag</li> </ul> <p>Hoodie Thundercats retro, tote bag 8-bit Godard craft beer gastropub. Truffaut Tumblr taxidermy, raw denim Kickstarter sartorial dreamcatcher. Quinoa chambray slow-carb salvia readymade, bicycle rights 90’s yr typewriter selfies letterpress cardigan vegan.</p> <hr/> <p>Pug heirloom High Life vinyl swag, single-origin coffee four dollar toast taxidermy reprehenderit fap distillery master cleanse locavore. Est anim sapiente leggings Brooklyn ea. Thundercats locavore excepteur veniam eiusmod. Raw denim Truffaut Schlitz, migas sapiente Portland VHS twee Bushwick Marfa typewriter retro id keytar.</p> <blockquote> We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another. —Anais Nin </blockquote> <p>Fap aliqua qui, scenester pug Echo Park polaroid irony shabby chic ex cardigan church-key Odd Future accusamus. Blog stumptown sartorial squid, gastropub duis aesthetic Truffaut vero. Pinterest tilde twee, odio mumblecore jean shorts lumbersexual.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="links"/><summary type="html"><![CDATA[march & april, looking forward to summer]]></summary></entry></feed>