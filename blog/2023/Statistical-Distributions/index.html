<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Statistical Distributions | Guiye Li</title> <meta name="author" content="Guiye Li"> <meta name="description" content=""> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://liguiye.github.io/blog/2023/Statistical-Distributions/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Guiye </span>Li</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Statistical Distributions</h1> <p class="post-meta">February 1, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/category/bayesian"> <i class="fas fa-tag fa-sm"></i> Bayesian,</a>   <a href="/blog/category/markov"> <i class="fas fa-tag fa-sm"></i> Markov</a>   </p> </header> <article class="post-content"> <p><em>TOC</em></p> <ul id="markdown-toc"> <li><a href="#basic-knowledge" id="markdown-toc-basic-knowledge">Basic knowledge</a></li> <li> <a href="#common-distributions" id="markdown-toc-common-distributions">Common distributions</a> <ul> <li><a href="#symmetric-probability-distribution" id="markdown-toc-symmetric-probability-distribution">Symmetric probability distribution</a></li> <li><a href="#uniform-distribution" id="markdown-toc-uniform-distribution">Uniform distribution</a></li> <li><a href="#bernoulli-distribution" id="markdown-toc-bernoulli-distribution">Bernoulli distribution</a></li> <li><a href="#binomial-distribution" id="markdown-toc-binomial-distribution">Binomial distribution</a></li> <li><a href="#negative-binomial-distribution" id="markdown-toc-negative-binomial-distribution">Negative binomial distribution</a></li> <li><a href="#geometric-distribution" id="markdown-toc-geometric-distribution">Geometric distribution</a></li> <li><a href="#poisson-distribution" id="markdown-toc-poisson-distribution">Poisson distribution</a></li> <li><a href="#exponential-distribution" id="markdown-toc-exponential-distribution">Exponential distribution</a></li> <li><a href="#gamma-distribution" id="markdown-toc-gamma-distribution">Gamma distribution</a></li> <li><a href="#inverse-gamma-distribution" id="markdown-toc-inverse-gamma-distribution">Inverse-gamma distribution</a></li> <li><a href="#beta-distribution" id="markdown-toc-beta-distribution">Beta distribution</a></li> <li><a href="#normal-distribution" id="markdown-toc-normal-distribution">Normal distribution</a></li> </ul> </li> </ul> <p><em>Reference:</em></p> <p>Mainly collected from Wikipedia and class notes of the class STAT 5100 in CU Boulder.</p> <h1 id="basic-knowledge">Basic knowledge</h1> <ol> <li> <p>Cumulative distribution function (<strong>CDF</strong>)</p> <p>The cumulative distribution function of a real-valued random variable \(X\) is the function given by</p> \[F_X (x) = P(X \leq x)\] <p>CDF can be find by integrating PDF.</p> </li> <li> <p>Probability density function (<strong>PDF</strong>)</p> <p>A function that defines the relationship between continuous random variables and their probabilities. If the random variables are discrete, we call it <strong>Probability mass function (PMF)</strong>.</p> \[P(X \in [a,b]) = \int_a^b f_X(x)dx\] <p>PDF can be find by differentiating CDF.</p> \[f(x) \geq 0 \quad , \quad \int_{-\infty}^{\infty}f(x)dx = 1\] <p>Continuous:</p> \[E[X] = \int_{-\infty}^{\infty}x \cdot f(x)dx\] <p>Discrete:</p> \[E[X] = \sum_x x \cdot \underbrace{P(X=x)}_\text{f(x)}\] </li> <li> <p>Law of total probability</p> \[P(A) = \sum_n P(A \cap B_n) \text{ or } P(A) = \sum_n P(A \mid B_n) P(B_n)\] </li> <li> <p>Bayesian statistics</p> \[\text{Posterior} = \frac{\text{Likelihood} \times \text{Prior}}{\text{Evidence}}\] <p>Given a prior belief that a PDF is \(p(\theta)\) and that the observations \(x\) have a likelihood \(p(x \mid \theta )\), then the posterior probability is defined as</p> \[p(\theta \mid x) = \frac{p(x \mid \theta) p(\theta)}{p(x)}\] <p>where \(p(\theta)\) is the normalizing constant and is calculated as</p> \[p(x) = \int p(x\mid\theta) p(\theta) d(\theta)\] </li> <li> <p>Conjugate prior</p> <p>In Bayesian probability, if the <strong><em>posterior</em></strong> distribution \(p ( \theta \mid x )\) is in the same probability distribution family as the <strong><em>prior</em></strong> probability distribution \(p(\theta )\), the <strong><em>prior</em></strong> and <strong>posterior</strong> are then called <strong>conjugate distributions</strong>, and the prior is called a <strong>conjugate prior</strong> for the <strong><em>likelihood</em></strong> function \(p(x\mid \theta )\).</p> <p><a href="https://en.wikipedia.org/wiki/Conjugate_prior#Table_of_conjugate_distributions" rel="external nofollow noopener" target="_blank">Common conjugate distributions</a></p> </li> <li> <p>Manually calculate a p-value</p> <p>The p-value for:</p> <p>a lower-tailed test is specified by: \(\text{p-value} = P(TS \leq ts \mid H_0 \text{ is true}) = \text{cdf}(ts)\)</p> <p>an upper-tailed test is specified by: \(\text{p-value} = P(TS \geq ts \mid H_0 \text{ is true}) = 1 - \text{cdf}(ts)\)</p> <p>\(\text{TS}\) is “Test statistic”, \(\text{ts}\) is the observed value of the test statistic calculated from your sample.</p> </li> <li> <p>Distributions of Certain Sums</p> <ul> <li>A Sum of Bernoullis is Binomial</li> <li>A Sum of Binomials is Binomial</li> <li>A Sum of Poissons is Poisson</li> <li>A Sum of Geometrics is Negative Binomial</li> </ul> </li> <li> <p>Things to know about \(e^x\)</p> <ul> <li>Lim: \(\lim_{n\rightarrow\infty}(1+\frac{x}{n})^n = e^x\)</li> <li>Taylor Series Expansion: \(e^x = \sum_{n=0}^\infty \frac{x^n}{n!} = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \ldots\)</li> </ul> </li> </ol> <h1 id="common-distributions">Common distributions</h1> <h2 id="symmetric-probability-distribution">Symmetric probability distribution</h2> <p>A probability distribution is said to be symmetric if and only if there exists a value \(x_{0}\) such that</p> \[f(x_{0}-\delta )=f(x_{0}+\delta )\] <p>for all real numbers \(\delta\) , where \(f\) is the PDF if the distribution is continuous or the probability mass function if the distribution is discrete.</p> <h2 id="uniform-distribution">Uniform distribution</h2> <p>Notation: \(X \sim \mathrm{Unif}(a,b)\)</p> <p>PDF:</p> \[f(x)={\begin{cases}{\frac {1}{b-a}}&amp;\mathrm {for} \ a\leq x\leq b,\\[8pt]0&amp;\mathrm {for} \ x&lt;a\ \mathrm {or} \ x&gt;b\end{cases}}\] <figure> <img src="https://upload.wikimedia.org/wikipedia/commons/9/96/Uniform_Distribution_PDF_SVG.svg" alt="PDF of continuous uniform distribution" width="300pt"> <figcaption>PDF of continuous uniform distribution</figcaption> </figure> <p>CDF:</p> \[f(x)={\begin{cases}0&amp;{\text{for }}x&lt;a\\[8pt]{\frac {x-a}{b-a}}&amp;{\text{for }}a\leq x\leq b\\[8pt]1&amp;{\text{for }}x&gt;b\end{cases}}\] <figure> <img src="https://upload.wikimedia.org/wikipedia/commons/6/63/Uniform_cdf.svg" alt="CDF of continuous uniform distribution" width="300pt"> <figcaption>CDF of continuous uniform distribution</figcaption> </figure> <h2 id="bernoulli-distribution">Bernoulli distribution</h2> <p>Notation: \(Bernoulli(p)\) or \(Bern(p)\)</p> <p>Let \(X\) be a random variable that takes on the values \(1\) and \(0\) with respective probabilities \(p\) and \(1 − p\). Then \(X\) is said to have a <em>Bernoulli distribution with parameter \(p\)</em>.</p> \[{\displaystyle \Pr(X=1)=p=1-\Pr(X=0)=1-q.}\] <p>The PMF of this distribution, over possible outcomes \(x\), is</p> \[P(X=x)={ \begin{cases} p &amp;{\text{if }}x=1,\\ q=1-p &amp;{\text{if }}x=0.\end{cases}}\] <p>or</p> \[{\displaystyle P(X=x)=p^{x}(1-p)^{1-x}\quad {\text{for }}x\in \{0,1\}}\] <p>or</p> \[{\displaystyle P(X=x)=px+(1-p)(1-x)\quad {\text{for }}x\in \{0,1\}.}\] <p>The mean or expected value of the Bernoulli distribution is</p> \[E[X] = \sum_x x P (X = x) = 0 (1 − p) + 1 p = p\] \[E[X^2] = \sum_x x P (X = x) = 0^2 (1 − p) + 1^2 p = p\] <p>Then the variance of this distribution is</p> \[\text{Var}[X] = E[X^2] − (E[X])^2 = p − p^2 = p(1 − p)\] <p>The Bernoulli distribution is a special case of the <a href="#binomial-distribution">binomial distribution</a> with \(n = 1\).</p> <h2 id="binomial-distribution">Binomial distribution</h2> <p>Notation: \(B(n,p)\) or \(bin(n,p)\)</p> <p>The binomial distribution with parameters \(n\) and \(p\) is the discrete probability distribution of <em>the number of successes in a sequence of \(n\) independent experiments</em>, each asking a yes–no question, and each with its own Boolean-valued outcome: success (with probability \(p\)) or failure (with probability \(q = 1 − p\)).</p> <p>PMF:</p> \[P(X=x)={\binom {n}{x}}p^{x}(1-p)^{n-x}\] <p>for x = 0, 1, 2, …, n, where</p> \[{\displaystyle {\binom {n}{x}}={\frac {n!}{x!(n-x)!}}}\] <p>is the binomial coefficient. The formula can be understood as follows: \(x\) successes occur with probability \(p^x\) and \(n − x\) failures occur with probability \((1-p)^{n-x}\). However, the \(x\) successes can occur anywhere among the \(n\) trials, and there are \({\tbinom {n}{x}}\) different ways of distributing \(x\) successes in a sequence of \(n\) trials.</p> \[E[X] = np\] \[Var[X] = np(1 − p)\] <p>A single success/failure experiment is also called a Bernoulli trial or Bernoulli experiment, and a sequence of outcomes is called a Bernoulli process; for a single trial, i.e., \(n = 1\), the binomial distribution is a <a href="#bernoulli-distribution">Bernoulli distribution</a>.</p> <h2 id="negative-binomial-distribution">Negative binomial distribution</h2> <p>Notation: \(\mathrm {NB} (r,\,p)\) or \(negbin(r,p)\)</p> <p>\(r &gt; 0\) — number of successes until the experiment is stopped (integer, but the definition can also be extended to reals)</p> <p>\(p \in [0,1]\) — success probability in each experiment (real)</p> <p>PMF:</p> \[{\displaystyle f(x;r,p)\equiv \Pr(X=x)={\binom {x+r-1}{x}}(1-p)^{x}p^{r}}\] <h2 id="geometric-distribution">Geometric distribution</h2> <p>Notation: \(geom(p)\)</p> <p>Consider a sequence of independent trials of an experiment where each trial can result in either “Success” (\(S\)) or “Failure” (\(F\)). Let \(0 \leq p \leq 1\) be the probability of <em>success</em> on any one trial. It’s a special case of negative binomial distribution.</p> <p>Definition 1 (“number of trials” model):</p> <p>Let \(x\) be the number of trials until the first success. There will be \(x-1\) failures, each with probability \(1-p\)</p> <p>PMF:</p> \[P (X = x) = (1 − p)^{x−1} p \text{ for } x \text{ in } \{1, 2, 3, \ldots \}\] <p>Definition 2 (“number of failures” model):</p> <p>Let \(x\) be the number of failures before the first success. There will be \(x\) failures, each with probability \(1-p\).</p> <p>PMF:</p> \[P (X = x) = (1 − p)^{x} p \text{ for } x \text{ in } \{0, 1, 2, \ldots \},\] <h2 id="poisson-distribution">Poisson distribution</h2> <p>Notation \(Poisson(\lambda)\)</p> <p>A random variable \(X\) has a Poisson distribution with parameter \(\lambda &gt; 0\) if \(X\) has PDF</p> \[\begin{aligned} P (X = x) &amp;= {\begin{cases} \frac{e^{−\lambda}\lambda^x}{x!} &amp;, \quad x = 0, 1, 2, \ldots\\ 0 &amp;, \quad \text{otherwise} \end{cases}} \\ &amp;= \frac{e^{−\lambda}\lambda^x}{x!} I_{\{0,1,2,\dots\}}(x) \end{aligned}\] <p>Expectation:</p> \[\begin{aligned} E[X] &amp;= \sum_x x \cdot P(X=x) \\ &amp;= \sum_{x=0}^{\infty} x \cdot \frac{e^{-\lambda}\lambda^{x}}{x!}\\ &amp;= \sum_{x=1}^{\infty} x \cdot \frac{e^{-\lambda}\lambda^{x}}{x!}\\ &amp;= \sum_{x=1}^{\infty} \frac{e^{-\lambda}\lambda^{x}}{(x-1)!}\\ &amp;= e^{-\lambda} \sum_{x=1}^{\infty} \frac{\lambda^{x}}{(x-1)!}\\ &amp;= \lambda e^{-\lambda} \sum_{x=1}^{\infty} \frac{\lambda^{x-1}}{(x-1)!}\\ &amp;= \lambda e^{-\lambda} \sum_{x=0}^{\infty} \frac{\lambda^{x}}{(x)!}\\ &amp;= \lambda e^{-\lambda} e^{\lambda}\\ &amp;= \lambda \end{aligned}\] <h2 id="exponential-distribution">Exponential distribution</h2> <p>Notation: \(X \sim exp(\lambda)\)</p> <p>Let \(\lambda &gt; 0\) be a fixed parameter and consider the continuous random variable with PDF</p> \[\begin{aligned} f(x) &amp;= {\begin{cases} \lambda e^{-\lambda x} &amp;, \quad x \geq 0\\ 0 &amp;, \quad x &lt; 0 \end{cases}} \\ &amp;= \lambda e^{-\lambda x} I_{[0,\infty)}(x) \end{aligned}\] <p>Claims:</p> <ol> <li>If \(X_1, X_2, \ldots , X_n \stackrel{iid}{\sim} exp(\lambda)\), \(\sum_{i=1}^{n}X_i \sim \Gamma(n, \lambda)\)</li> <li>If \(y = min (X_1, \ldots, X_n)\), \(y \sim exp(n\lambda)\)</li> </ol> <p>CDF:</p> \[F (x) = P (X \leq x) = {\begin{cases} \int_o^x \lambda e^{−\lambda u} du = 1 − e^{−λx} &amp;, \quad x \geq 0 \\ 0 &amp;, \quad x &lt; 0 \end{cases}}\] <p>Expectation:</p> \[\begin{aligned} E[X] &amp;= \int_{-\infty}^{\infty}x \cdot f(x)dx \\ &amp;= \int_{-\infty}^0 x \cdot 0 \cdot dx + \int_0^{\infty}x\cdot\lambda e^{-\lambda x}dx \\ &amp;= \frac{1}{\lambda} \end{aligned}\] <p>Tail probability:</p> \[\bar{F} (x) = P (X &gt; x) = 1 − P (X \leq x) = 1 − F (x) = e^{−λx}.\] <p>The exponential distribution is the only continuous distribution with the lack of memory property.</p> \[\bar{F}(x+y) = \bar{F}(x)\cdot\bar{F}(y)\] <h2 id="gamma-distribution">Gamma distribution</h2> <p>Notation:</p> <ul> <li>Gamma distribution \(\Gamma(\alpha, \beta)\)</li> <li>Gamma function \(\Gamma(\alpha)\)</li> </ul> <p>Gamma distribution:</p> <p>Let \(\alpha &gt; 0\) and \(\beta &gt; 0\) be fixed parameters and consider the continuous random variable with PDF</p> \[\begin{aligned} f(x) &amp;= {\begin{cases} \frac{1}{\Gamma(\alpha)}\beta^{\alpha}x^{\alpha-1}e^{-\beta x} &amp;, \quad x &gt; 0\\ 0 &amp;, \quad \text{otherwise} \end{cases}} \\ &amp;= \frac{1}{\Gamma(\alpha)}\beta^{\alpha}x^{\alpha-1}e^{-\beta x} I_{(0,\infty)}(x) \end{aligned}\] <p>Note that</p> \[\begin{aligned} \int_0^{\infty} \beta^{\alpha}x^{\alpha-1}e^{-\beta x}dx &amp;= \int_0^{\infty}(\beta x)^{\alpha-1}e^{-\beta x}\beta dx &amp;, \quad \text{Let } u = \beta x, u : 0 \rightarrow \infty \\ &amp;= \int_0^{\infty}u^{\alpha-1}e^{-u}du \\ &amp;= \Gamma(\alpha) \end{aligned}\] <p>\(\alpha\) is known as a <em>shape parameter</em> and \(\beta\) is known as an <em>inverse scale parameter</em>.</p> <p>Gamma function:</p> <p>The pdf is given in terms of the <em>gamma function</em> which is defined, for \(\alpha &gt; 0\) as \(\Gamma(\alpha) := \int_0^{\infty} x^{\alpha -1} e^{-x}dx\). (\(:=\) is “defined as”)</p> <p>Properties of Gamma function:</p> <ul> <li>For \(\alpha &gt;1\), \(\Gamma(\alpha) = (\alpha-1)\Gamma(\alpha-1)\)</li> <li>If \(n\geq 1\) is an integer, \(\Gamma(n) = (n-1)!\)</li> <li>\(\Gamma(1)=1\) since \(0!=1\) or \(\Gamma(1) = \int_0^{\infty} x^{1 -1} e^{-x}dx = \int_0^{\infty} e^{-x}dx = 1\)</li> </ul> <h2 id="inverse-gamma-distribution">Inverse-gamma distribution</h2> <p>Notation: \(X \sim \mathrm{Inv}\Gamma(\alpha, \beta)\)</p> \[{\displaystyle f(x;\alpha ,\beta )={\frac {\beta ^{\alpha }}{\Gamma (\alpha )}}(\frac{1}{x})^{\alpha +1}e^{\left(-\beta \frac{1}{x}\right)}} \text{ , for } x &gt; 0\] <h2 id="beta-distribution">Beta distribution</h2> <p>Notation: \(Beta(\alpha, \beta)\)</p> <p>A family of continuous probability distributions defined on the interval \([ 0 , 1 ]\) in terms of two positive parameters, denoted by alpha (\(\alpha\)) and beta (\(\beta\)), that appear as exponents of the variable and its complement to \(1\), respectively, and control the shape of the distribution.</p> <p>Mean:</p> \[E[X] = \frac{\alpha}{\alpha + \beta}\] <p>PDF:</p> \[\begin{align*} f(p;\alpha, \beta) &amp;= \frac{(\alpha + \beta -1)!}{(\alpha -1)!(\beta-1)!} p^{\alpha-1}(1-p)^{\beta-1} \\ &amp;\propto p^{\alpha-1}(1-p)^{\beta-1} \end{align*}\] <figure> <img src="https://upload.wikimedia.org/wikipedia/commons/f/f3/Beta_distribution_pdf.svg" alt="PDF of Beta distribution" width="300pt"> <figcaption>PDF of Beta distribution</figcaption> </figure> <p>CDF:</p> <figure> <img src="https://upload.wikimedia.org/wikipedia/commons/1/11/Beta_distribution_cdf.svg" alt="CDF of Beta distribution" width="300pt"> <figcaption>CDF of Beta distribution</figcaption> </figure> <p>In Bayesian inference, the beta distribution is the <a href="#basic-knowledge">conjugate prior</a> probability distribution for the <a href="#bernoulli-distribution">Bernoulli</a>, <a href="#binomial-distribution">binomial</a>, <a href="#negative-binomial-distribution">negative binomial</a> and <a href="#geometric-distribution">geometric distributions</a>.</p> <h2 id="normal-distribution">Normal distribution</h2> <p>Notation: \(X \sim \mathcal{N}(\mu, \sigma^2)\)</p> <p>PDF:</p> \[{\displaystyle f(x)={\frac {1}{\sigma {\sqrt {2\pi }}}}e^{-{\frac {1}{2}}\left({\frac {x-\mu }{\sigma }}\right)^{2}}}\] </article><div id="giscus_thread" style="max-width: 900px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"LiGuiye/LiGuiye.github.io","data-repo-id":"R_kgDOInph-Q","data-category":"Announcements","data-category-id":"DIC_kwDOInph-c4CTGBO","data-mapping":"title","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Guiye Li. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-50GRRBDLH3"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-50GRRBDLH3");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>