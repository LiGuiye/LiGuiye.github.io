<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Diffusion models | Guiye Li</title> <meta name="author" content="Guiye Li"> <meta name="description" content=""> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://liguiye.github.io/blog/2022/Diffusion-models/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Guiye </span>Li</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Diffusion models</h1> <p class="post-meta">December 14, 2022</p> <p class="post-tags"> <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/category/generative-model"> <i class="fas fa-tag fa-sm"></i> generative-model</a>   <a href="/blog/category/deep-learning"> <i class="fas fa-tag fa-sm"></i> deep-learning</a>   <a href="/blog/category/diffusion-model"> <i class="fas fa-tag fa-sm"></i> diffusion-model</a>   </p> </header> <article class="post-content"> <p><em>TOC</em></p> <ul id="markdown-toc"> <li> <a href="#background-information" id="markdown-toc-background-information">Background information</a> <ul> <li><a href="#markov-chain" id="markdown-toc-markov-chain">Markov chain</a></li> <li><a href="#reparameterization-trick" id="markdown-toc-reparameterization-trick">Reparameterization trick</a></li> </ul> </li> <li> <a href="#main-idea-of-diffusion-model" id="markdown-toc-main-idea-of-diffusion-model">Main idea of Diffusion Model</a> <ul> <li><a href="#forward-process-or-diffusion-process" id="markdown-toc-forward-process-or-diffusion-process"><em>Forward process</em> (or <em>diffusion process</em>)</a></li> <li><a href="#reverse-process-or-reverse-diffusion-process" id="markdown-toc-reverse-process-or-reverse-diffusion-process"><em>Reverse process</em> (or <em>reverse diffusion process</em>)</a></li> </ul> </li> <li><a href="#benefits-of-diffusion-models" id="markdown-toc-benefits-of-diffusion-models">Benefits of Diffusion Models</a></li> <li><a href="#training" id="markdown-toc-training">Training</a></li> <li><a href="#summary" id="markdown-toc-summary">Summary</a></li> </ul> <p><em>Reference:</em></p> <p>Official links:</p> <ol> <li> <p>DDPM (NeurIPS 2020)</p> <p>Paper: <a href="https://arxiv.org/abs/2006.11239" rel="external nofollow noopener" target="_blank">Denoising Diffusion Probabilistic Models</a></p> <p>Github: <a href="https://github.com/hojonathanho/diffusion" rel="external nofollow noopener" target="_blank">https://github.com/hojonathanho/diffusion</a></p> <p>Website: <a href="https://hojonathanho.github.io/diffusion/" rel="external nofollow noopener" target="_blank">https://hojonathanho.github.io/diffusion/</a></p> </li> <li> <p>Improved DDPM (ICML 2021)</p> <p>Paper: <a href="https://arxiv.org/abs/2102.09672" rel="external nofollow noopener" target="_blank">Improved Denoising Diffusion Probabilistic Models</a></p> <p>Github: <a href="https://github.com/openai/improved-diffusion" rel="external nofollow noopener" target="_blank">https://github.com/openai/improved-diffusion</a></p> </li> <li> <p>Guided Diffusion Models (NeurIPS 2021)</p> <p>Paper: <a href="https://arxiv.org/pdf/2105.05233.pdf" rel="external nofollow noopener" target="_blank">Diffusion Models Beat GANs on Image Synthesis</a></p> <p>Github: <a href="https://github.com/openai/guided-diffusion" rel="external nofollow noopener" target="_blank">https://github.com/openai/guided-diffusion</a></p> </li> </ol> <p>Blog:</p> <ol> <li><a href="https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/" rel="external nofollow noopener" target="_blank">Introduction to Diffusion Models for Machine Learning</a></li> <li><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/" rel="external nofollow noopener" target="_blank">What are Diffusion Models?</a></li> </ol> <h2 id="background-information">Background information</h2> <h3 id="markov-chain">Markov chain</h3> <p>Quote from <a href="https://en.wikipedia.org/wiki/Markov_chain#:~:text=A%20Markov%20chain%20or%20Markov,the%20state%20of%20affairs%20now.%22" rel="external nofollow noopener" target="_blank">Wikipedia</a>:</p> <p>A <strong>Markov chain</strong> or <strong>Markov process</strong> is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. Informally, this may be thought of as, “What happens next depends only on the state of affairs now”.</p> <h3 id="reparameterization-trick">Reparameterization trick</h3> <p>The role of reparameterization trick (from VAE) is to make a stochastic sampling process trainable. Assuming a sampling process from \(\mathbf{z} \sim q_ \phi(\mathbf{z}\vert\mathbf{x})\). To express the random variable \(\mathbf{z}\) as a deterministic variable \(\mathbf{z} = \mathcal{T}_ \phi (\mathbf{x}, \mathbf{\epsilon} )\), where \(\mathbf{\epsilon}\) is an suxiliary independent random variable, and the transformation function \(\mathcal{T}_ \phi\) parameterized by \(\phi\) converts \(\mathbf{\epsilon}\) to \(\mathbf{z}\).</p> <p>For example, a common choice of the form of \(q_ \phi (z \vert x)\) is a multivariate Gaussian distribution with a diagonal covariance structure:</p> <p>\begin{equation} \mathbf{z} \sim q_\phi(\mathbf{z}\vert\mathbf{x}^{(i)}) = \mathcal{N}(\mathbf{z}; \boldsymbol{\mu}^{(i)}, \boldsymbol{\sigma}^{2(i)}\boldsymbol{I}) \end{equation}</p> <p>Applying the reparameterization trick:</p> <p>\begin{equation} \mathbf{z} = \boldsymbol{\mu} + \boldsymbol{\sigma} \odot \boldsymbol{\epsilon} \quad \text{where} \quad \boldsymbol{\epsilon} \sim \mathcal{N}(0, \boldsymbol{I}) \end{equation}</p> <p>Tips:</p> <ol> <li>\(\odot\) refers to element-wise product.</li> <li>\(q_ \phi (\mathbf{z} \vert \mathbf{x})\) stands for a estimated posterior probability function, aso known as <strong>probabilistic encoder</strong>.</li> <li>\(p_{\theta}(\mathbf{x}\vert\mathbf{z})\) is the likelihood of generating true data sample given the latent code, also known as <strong>probabilistic decoder</strong>.</li> </ol> <h2 id="main-idea-of-diffusion-model">Main idea of Diffusion Model</h2> <p>Diffusion models works by <strong>destroying training data</strong> through the successive addition if Gaussian noise, and then <strong>learning to recover</strong> the data by reversing this noising process. After training, we can use the Diffusion Model to generate data by simply <strong>passing randomly sampled noise through the learned denoising process</strong>.</p> <h3 id="forward-process-or-diffusion-process"> <em>Forward process</em> (or <em>diffusion process</em>)</h3> <p>Specifically, a Diffusion Model is a latent variable model which maps to the latent space using a fixed Markov chain. This chain gradually adds noise to the data in order to obtain the <strong>approximate posterior</strong> \(q(\mathbf{x}_{1:T}\vert \mathbf{x}_0)\), where \(\mathbf{x}_1,\ldots,\mathbf{x}_T\) are the latent variables (a sequence of noisy samples) with the same dimensionality as \(\mathbf{x}_0\). See figure below.</p> <p><img src="https://www.assemblyai.com/blog/content/images/size/w1000/2022/05/image.png" alt="The Markov chain manifested for image data." width="600pt"></p> <p>A parameterization of the forward process (combing Markov assumption):</p> <p>\begin{equation} \label{eq:forward-process} q(\mathbf{x}_ t \vert \mathbf{x}_ {t-1}) = \mathcal{N}(\mathbf{x}_ t; \sqrt{1 - \beta_ t} \mathbf{x}_ {t-1}, \beta_ t\mathbf{I}) \quad q(\mathbf{x}_ {1:T} \vert \mathbf{x}_ 0) = \prod^T_{t=1} q(\mathbf{x}_ t \vert \mathbf{x}_ {t-1}) \end{equation}</p> <p>where \(\{\beta_t \in (0, 1)\}_{t=1}^T\) is a variance schedule (either learned or fixed) controlling the step sizes which, if well-behaved, <strong>ensures that \(\mathbf{x}_T\) is nearly an isotropic Gaussian for sufficiently large \(T\)</strong>. In other words, the data sample \(\mathbf{x}_0\) gradually loss its distinguishable features as the step \(t\) becomes larger. Eventually, when \(T \to \infty\), \(\textbf{x}_T\) is equivalent to an isotropic Gaussian distribution.</p> <p>We can sample \(\mathbf{x}_t\) at any arbitrary time step \(t\) in a closed form using <a href="#reparameterization-trick">reparameterization trick</a>.</p> <p>Let \(\alpha_t = 1 - \beta_ t\) and \(\bar{\alpha}_t = \prod_{i=1}^t \alpha_i\)</p> \[\begin{align} \mathbf{x}_ t &amp;= \sqrt{1 - \beta_ t} \mathbf{x}_ {t-1} + \sqrt{\beta_ t} \epsilon_ {t-1} \quad \text{, where } {\epsilon_ t \sim \mathcal{N}(\mathbf{0},\mathbf{I})}_ {t=0}^{t-1} \\ &amp;= \sqrt{\alpha_t} \mathbf{x}_ {t-1} + \sqrt{1-\alpha_ t} \epsilon_ {t-1} \\ &amp;= \sqrt{\alpha_t \alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{1 - \alpha_t \alpha_{t-1}} \bar{\boldsymbol{\epsilon}}_{t-2} \quad \text{, where } \bar{\boldsymbol{\epsilon}}_{t-2} \text{ merges two Gaussian distributions.} \end{align}\] <p>Tips:</p> <ol> <li>If we merge two Gaussian distributions with different variance, \(\mathcal{N}(\mathbf{0}, \sigma^2_ 1 \mathbf{I})\) and \(\mathcal{N}(\mathbf{0}, \sigma^2_ 2 \mathbf{I})\), the new distribution is \(\mathcal{N}(\mathbf{0}, (\sigma^2_ 1 + \sigma^2_ 2) \mathbf{I})\).</li> </ol> <h3 id="reverse-process-or-reverse-diffusion-process"> <em>Reverse process</em> (or <em>reverse diffusion process</em>)</h3> <p>Ultimately, the image is asymptotically transformed to pure Gaussian noise. The goal of training a diffusion model is to <strong>learn the reverse process</strong>, i.e. training \(p_\theta (X_{t-1} \vert X_t)\). See figure below, by traversing backwards along this chain, we can generate new data.</p> <p><img src="https://www.assemblyai.com/blog/content/images/size/w1000/2022/05/image-1.png" alt="The reverse process of the Markov chain." width="600pt"></p> <p>Starting with the pure Gaussian noise \(p(X_T)=\mathcal{N}(X_T;\mathbf{0},\mathbf{I})\), the model learns the joint distribution \(p_\theta(X_0;T)\) as:</p> <p>\begin{equation} p_ \theta ( \mathbf{x}_ {t-1} \vert \mathbf{x}_ t) = \mathcal{N} ( \mathbf{x}_ {t-1}; \boldsymbol{\mu}_ \theta ( \mathbf{x}_ t, t ) , \boldsymbol{\Sigma}_ \theta ( \mathbf{x}_ t, t ) ) \quad p_ \theta ( \mathbf{x}_ {0:T} ) = p ( \mathbf{x}_ T ) \prod^T_ {t=1} p_ \theta ( \mathbf{x}_ {t-1} \vert \mathbf{x}_ t ) \end{equation}</p> <p>where the time-dependent parameters of the Gaussian transitions are learned.</p> <h2 id="benefits-of-diffusion-models">Benefits of Diffusion Models</h2> <ol> <li>Diffusion Models currently produce State-of-the-Art image quality.</li> <li>Not requiring adversarial training.</li> <li>Scalability and parallelizability.</li> </ol> <h2 id="training">Training</h2> <p>A Diffusion Model is trained by <strong>finding the reverse Markov transitions that maximize the likelihood of the training data</strong>. In practice, training equivalently consists of minimizing the variational upper bound on the negative log likelihood.</p> <p>\begin{equation} \mathbb{E} [= \log p_\theta (X_0)] \leqslant \mathbb{E}_ {q} [-\log \frac{p_\theta (X_{0:T})}{q(X_{1:T} \vert X_0)}] =: L_{vlb} \end{equation}</p> <p>Variational lower bound \(L_{vlb}\) is technically an upper bound (the negative of the Evidence Lower Bound (ELBO)) which we are trying to minimize. We will try to rewrite the \(L_{vlb}\) in terms of Kullback-Leibler (KL) Divergences because the transition distributions in the Markov chain are Gaussians, and <strong>the KL divergence between Gaussians has a closed form</strong>.</p> <p>\begin{equation} D_{KL}(P\parallel Q) = \int_{-\infty}^{\infty} p(x)\log(\frac{p(x)}{q(x)}) dx \end{equation}</p> <p>Casting \(L_{vlb}\) in terms of KL Divergences</p> <p>\begin{equation} L_{vlb} := L_0 + L_1 + \ldots + L_{T-1} + L_T \end{equation}</p> <p>where</p> <p>\begin{equation} L_0 := -\log p_\theta(X_0 \vert X_1) \end{equation}</p> <p>\begin{equation} L_{t-1} := D_{KL}(q(X_{t-1} \vert X_t,X_0)\parallel p_\theta(X_{t-1} \vert X_t)) \end{equation}</p> <p>\begin{equation} L_T := D_{KL}(q(X_T \vert X_0)\parallel p(X_T)) \end{equation}</p> <p><em>Let’s skip the proof for now.</em></p> <p>Conditioning the forward process posterior on \(X_0\) in \(L_{t-1}\) results in a tractable form that leads to <strong>all KL divergences being comparisons between Gaussians</strong>. This means that the divergences can be exactly calculated with closed-form expressions rather than with Monte Carlo estimates.</p> <h2 id="summary">Summary</h2> <ol> <li>Diffusion Models are <strong>highly flexible</strong> and allow for any architecture whose input and output dimensionality are the same to be used. Many implementations use <strong>U-Net-like</strong> architectures.</li> <li>The <strong>training objective</strong> is to maximize the likelihood of t he training data. This is manifested as tuning the model parameters to <strong>minimize the variational upper bound of the negative log likelihood of the data</strong>.</li> <li>Almost all terms in the objective function can be cast as <strong>KL Divergences</strong> as a result of the Markov assumption. This values <strong>become tenable to calculated</strong> given that we are using Gaussians, therefore omitting the need to perform MonteCarlo approximation.</li> <li>A discrete decoder is used to obatin log likelihoods across pixel values as the last step in the reverse diffusion process.</li> </ol> </article><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"LiGuiye/LiGuiye.github.io","data-repo-id":"R_kgDOInph-Q","data-category":"Announcements","data-category-id":"DIC_kwDOInph-c4CTGBO","data-mapping":"title","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2022 Guiye Li. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-50GRRBDLH3"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-50GRRBDLH3");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>